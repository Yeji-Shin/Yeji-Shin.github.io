---
layout: post
title: StyleGAN
date: 2021-06-07
category: GAN
use_math: true
---


## A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)

오늘은 NVDIA에서 발표한 styleGAN을 살펴볼 것이다. 그에 앞서 StyleGAN의 baseline이 되는 PGGAN을 간단하게 살펴보도록 한다.

### 0. Progressive Growing of GANs for Improved Quality, Stability, and Variation (PGGAN) 

PGGAN은 아주 낮은 화질 (4x4)에서 시작해 점차적으로 더 높은 화질을 만들수 있는 레이어를 쌓아가며 고화질 이미지(1024x1024)를 생성한다.
초반에는 낮은 화질의 이미지에서도 볼 수 있는 기본적인 특징들을 학습해 이미지의 윤곽을 잡아주고, 화질을 높여가며 디테일 한 부분들을 학습해 나가는 것이다. 
이렇게 하면 저화질의 이미지가 고화질 이미지 학습의 토대가 되어 어느 정도의 스케치를 제시해주기 때문에 학습이 빨라진다는 장점이 있다. 부전승 느낌..?

![image](https://user-images.githubusercontent.com/61526722/120967339-4cad7e80-c7a2-11eb-868f-344d9187587e.png)

하지만 PGGAN은 다른 GAN 모델과 같이 학습의 내부 동작을 사람이 알 수 없기 때문에 이미지의 구체적인 특징을 조절하는 것이 어렵고, latent space의 속성도 컴퓨터가 아는 방식으로만 되어있어 해석이 어렵다.
그리고 특징들이 얽혀있기 때문에 인풋을 조금 변경하면 얽혀있는 여러 특징들에 동시에 변하기 때문에 원하는대로 조절이 힘들다.
따라서 NVIDA는 PGGAN의 generator의 구조를 새롭게 만들어서 이미지 합성 프로세스를 제어하는 styleGAN을 제안한다. (discriminator와 loss function을 전혀 건드리지 않는다.)

### 1. StyleGAN overview

위에서 PGGAN은 순차적으로 layer를 쌓아 고화질의 이미지를 만든다고 했다. StyleGAN은 PGGAN과 비슷하게 각 convolutional layer에 특정 스타일의 latent code를 적용하여 각기 다른 이미지 사이즈에서 서로 다른 스타일이 적용되도록 했다. 

![image](https://user-images.githubusercontent.com/61526722/120959328-765fa900-c794-11eb-8f58-40af8875ecc9.png)

- Coarse(굵직한 특징) 4x4 ~ 8x8 해상도: 포즈, 일반적인 헤어스타일, 얼굴형 등

- Middle(중간 특징) 16x16 ~ 32x32 해상도: 자세한 얼굴 특징, 헤어스타일, 눈 뜨고/감음 등

- Fine(자세한 특징) 64x64 ~ 1024x1024 해상도: 눈, 머리, 피부의 색 등

여기서 주목할 점은 latent code가 바로 generator의 입력으로 들어가는 것이 아니라 mapping network를 거쳐 나온 상수값(learned constant input)을 generator에 넣어준다는 것이다. 이 상수값은 가우시안 노이즈와 결합되어 convolution layer로 들어가고 unsupervised learning을 가능하게 한다. 아래 그림은 vanilla GAN과 styleGAN의 generator 구조이다. 

![image](https://user-images.githubusercontent.com/61526722/120959512-e2daa800-c794-11eb-98ac-04d8b096d639.png)
![image](https://user-images.githubusercontent.com/61526722/120966420-2c30f480-c7a1-11eb-9fff-08145c99be60.png)


그림에서 볼 수 있듯이 vanila GAN은 latent code $z$가 그대로 입력으로 들어가 up-sampling을 통해 이미지를 생성한다. 그에 반해 styleGAN은 latent code $z$를 8개의 FC layer로 이루어진 <mark>mapping network</mark>를 통해 $w$ (스타일을 의미하는 벡터)라는 intermediate vector로 변경한 후 이를 각 레이어 사이의 <mark>AdaIN</mark>이라는 정규화 레이어에 대입하면 각 레이어의 출력물에 스타일이 적용되는 것이다.

#### 2. Mapping Network

Mapping network는 'featrue entanglement' 문제를 해결하기 위해 특정 공간에서의 벡터를 다른 공간에서의 벡터로 바꿔주는 것이다. Featrue entanglement는 feature들이 얽혀있다는 의미이다. GAN은 아래 그림처럼 feature들이 서로 얽혀있어서 input latent vector를 변경하면 얽힌 여러가지의 feature들이 동시에 변하여 원하는 대로 이미지를 조절하는 것이 힘들다. 

![image](https://user-images.githubusercontent.com/61526722/120962324-7793d480-c79a-11eb-94d7-0a9c59887484.png)

출처: https://comlini8-8.tistory.com/11

따라서 이미지를 내 마음대로 조절하기 위해 latent vector를 서로 얽혀있지 않는 형태로 변경하는 작업이 필요하다. Mapping network는 latent vector를 각기 다른 시각적 특징을 다른 요소로 컨트롤할 수 있는 중간(intermediate) 벡터로 인코딩해준다. 여기서 latent vector $z$는 1x512 사이즈를 가지므로 8개의 FC layer를 거친 intermediate vector $w$의 사이즈도 1x512가 된다. 

![image](https://user-images.githubusercontent.com/61526722/120969784-8cc23080-c7a5-11eb-839d-14f0f87de16b.png)



#### 3. Style Modules (AdaIN)

Adaptive Instance Normalization (AdaIN)모듈은 매핑 네트워크에서 생성된 intermediate vector $w$를 이미지로 전달한다. 이 모듈은 convolutional layer뒤에 더해지고 해당 단계에서의 시각적인 표현을 정의한다. 

![image](https://user-images.githubusercontent.com/61526722/120968893-75367800-c7a4-11eb-9d48-812de3b12eb5.png)

총 9개의 block이 존재하고 각 block은 2개의 conv layer와 두개의 AdaIN layer로 구성된다. AdaIN은 convolution layer의 결과를 처리하기 위해 사용된다. 


#### 4. Removing Traditional Input

![image](https://user-images.githubusercontent.com/61526722/120970160-05c18800-c7a6-11eb-9bcf-db447d26248f.png)

latent vector로 시작하지 않아도 좋은 결과를 낼수 있다. 초기 입력을 상수로 대체한다. 
